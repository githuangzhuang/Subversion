环境准备镜像源
软件 vsftpd
mkdir /var/ftp/ceph
mount -o loop rhcs..iso  /var/ftp/ceph
启服务 vsftpd

2.yum源配三种 MON、OSD、Tools
3.解析文件 /etc/hosts   #必须与本机一致
4.配置无密码连接,私钥
ssh-keygen -f /root/.ssh/id_rsa -N ''
for i in 10 11..;do;ssh-copy-id 192.168.4.$i;done

5.配置NTP时间同步，启服务chronyd

部署ceph集群
软件 ceph-deploy
创建目录 mkdir /root/ceph-cluster  #以下操作在目录入完成
创建ceph集群  ceph-deploy new node1 node2 node3
所有节点装软件 ceph-deploy install node1 node2 node3
初始化mon服务  ceph-deploy mon create-initial

若出现以下错误
#[node1][ERROR ] admin_socket: exception getting command descriptions: [Error 2] No such file or directory
解决方案： vim /root/ceph-cluster/ceph.conf 
追加          public_network = 本服务器的ip网段
推送所有服务器：      ceph-deploy --overwrite-conf config push node1 node2 node3

创建OSD（所以服务器）
1.准备磁盘分区，作为存储服务器的日志journal盘,为其它两个磁盘缓存 vdc、vdd
更改所主属  chown ceph.ceph /dev/vdb1
          chown ceph.ceph /dev/vdb2
2.初始化清空磁盘数据（仅生成密钥服务器）
ceph-deploy disk zap node1:vdc node1:vdd
ceph-deploy disk zap node2:vdc node2:vdd
ceph-deploy disk zap node3:vdc node3:vdd

3.创建OSD存储空间（仅生成密钥服务器）
ceph-deploy osd create node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
ceph-deploy osd create node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
cdph-deploy osd create node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2

#以上操作若提示“run gatherkeys”错误提示下面命令修复：
  ceph-deploy gatherkeys node1 node2 node3
查看集群状态  ceph -s
#若出现 health:HEALTH_WARN  
         clock skew detected on node1,..
重启ceph服务
  systemctl restart ceph\*.service ceph\*.target

创建ceph块存储
查看存储池     ceph osd lspools
1.创建镜像
   rbd create demo-image --image-feature layering --size 10G
   rbd create rbd/image --image-feature layering --size 10G
   rbd list
rbd info demo-image

2.调整磁盘大小
缩小  rbd resize --size 7G image --allow-shrink
扩大  rbd resize --size 15G image
查看  rbd info image

集群内将镜像映射为本地磁盘
     rbd map demo-image
格式化 mkfs.xfs /dev/rbd0
挂载   mount /dev/rbd0  /mnt

客户端
1.安装软件 ceph-common 
2.拷贝服务器配置文件 /etc/ceph     #rbdmap不用
3.生成磁盘   rbd map image
4.格式化、挂载

创建镜像快照及还原快照
1.创建镜像快照  rbd snap create image --snap image-snap1
#创建之前客户端挂载目录必须先有信息,镜像名建议是有时间 image-snap09-12
查看镜像    rbd snap ls image

2.若客户端挂载目录内数据删除还原快照
服务端：  rbd snap rollback image --snap image-snap1
客户端重新挂载     umount /mnt;   mount /dev/rbd0 /mnt

创建快照克隆
 rbd snap protect image --snap image-snap1
 rbd clone image --snap image-snap1 image-clone --image-feature layering


